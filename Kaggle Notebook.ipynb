{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk, re, string, time, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textClassification import textAnalysis\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics \n",
    "from sklearn import cross_validation\n",
    "english_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# automate creation of csv for kaggle submission\n",
    "def create_submission(filename):\n",
    "    pd.DataFrame.to_csv(df_test[['Id','Category']], path_or_buf=filename, header=['Id','Category'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# yell outloud 'process complete'\n",
    "def finished():\n",
    "    os.system(\"say 'process complete'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# automate crossvalidation and printing of results\n",
    "def crossvalidation(model, train_features, real_category):\n",
    "    scores = cross_validation.cross_val_score(model, train_features, real_category, cv=5)\n",
    "    print(\"Accuracy: %0.5f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"newtrain.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"newtest.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allText = \"\"\n",
    "for i in df.Text:\n",
    "    allText += i + '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('allText.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.write(allText)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing functions from Text Analysis assignement on Yahoo questions text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"allText.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analysis = textAnalysis(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NP parsing.\n",
      "Done with NP parsing.\n",
      "Done with FD of NPs\n",
      "Extracting nouns...\n",
      "Done extracting nouns\n",
      "Done running Lesk on nouns\n",
      "Done creating FD on lemmas\n",
      "\n",
      "Most frequent noun phrases:\n",
      "---------------------------\n",
      ".how do                 91   \n",
      "br & gt                 27   \n",
      ".what do                22   \n",
      "my boyfriend            19   \n",
      "a guy                   19   \n",
      "my comput               16   \n",
      "the world               15   \n",
      "a girl                  14   \n",
      ".whi do                 11   \n",
      ".i am                   9    \n",
      ".i want                 9    \n",
      "my husband              9    \n",
      "\n",
      "\n",
      "Most frequent lemmas from NPs and their hyponyms:\n",
      "-------------------------------------------------\n",
      "can             194      (containerful.n.01)\n",
      "get             184      (return.n.11)\n",
      "know            156      (knowing.n.01)\n",
      "find            89       (act.n.02)\n",
      "need            77       (psychological_feature.n.01)\n",
      "want            70       (need.n.01)\n",
      "people          58       (group.n.01)\n",
      "someone         51       (organism.n.01)\n",
      "someone         51       (causal_agent.n.01)\n",
      "help            50       (activity.n.01)\n",
      "use             43       (custom.n.01)\n",
      "way             42       (category.n.02)\n",
      "\n",
      "\n",
      "Most frequent hyponyms:\n",
      "-----------------------\n",
      "person.n.01                    8    \n",
      "activity.n.01                  7    \n",
      "time_period.n.01               7    \n",
      "collection.n.01                4    \n",
      "computer.n.01                  3    \n",
      "woman.n.01                     3    \n",
      "content.n.05                   3    \n",
      "atmosphere.n.01                3    \n",
      "language_unit.n.01             3    \n",
      "location.n.01                  3    \n",
      "legal_document.n.01            3    \n",
      "physical_phenomenon.n.01       3    \n"
     ]
    }
   ],
   "source": [
    "# tokenize sentences and words an run them through a POS tagger\n",
    "sentences,tagged_sents = analysis.gen_corpus_prep()\n",
    "\n",
    "# generate noun phrases and proper nouns\n",
    "nps,nps_strings,fd_nps = analysis.chunker(sentences,tagged_sents)\n",
    "\n",
    "# deduce synsets through lesk algorithm and calculate distance between synsets\n",
    "final_list,hyponyms = analysis.lesk_comparison(nps,sentences)\n",
    "\n",
    "# print final report\n",
    "analysis.final_report(fd_nps,final_list,hyponyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_dict = {1:\"Business&Finance\",2:\"Computers&Internet\",3:\"Entertainment&Music\",4:\"Family&Relationships\",5:\"Education&Reference\",6:\"Health\",7:\"Science&Mathematics\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating feature for lengths of text and converting to pandas series\n",
    "lengths = []\n",
    "for i in df.Text.iteritems():\n",
    "    lengths.append(len(i[1]))\n",
    "lengths = pd.Series(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# appending lengths feature to dataframe\n",
    "df['lengths'] = lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# aggregate all text\n",
    "all_text = []\n",
    "for i in df.Text:\n",
    "    all_text.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# aggregate all text and tokenize it\n",
    "tok_sent = []\n",
    "for i in df.Text:\n",
    "    tok_sent.append(nltk.word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pos_tag every word\n",
    "pos_sent = []\n",
    "for i in tok_sent:\n",
    "    pos_sent.append(nltk.pos_tag(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation and Testing of Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Strategy: Count occurrence of every tag (17.6%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating list with all tags from upenn_tagset\n",
    "tagset = ['$', \"''\", '(', ')', ',', '--', '.', ':','CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating columns of zeroes for every possible tag\n",
    "for i in tagset:\n",
    "    df[i] = pd.Series(np.zeros(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenizing and tagging every question training set\n",
    "tagged_questions = []\n",
    "for i in df.Text:\n",
    "    tagged_questions.append(nltk.pos_tag(nltk.word_tokenize(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenizing and tagging every question for test set\n",
    "tagged_questions_test = []\n",
    "for i in df_test.Text:\n",
    "    tagged_questions_test.append(nltk.pos_tag(nltk.word_tokenize(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolassoldi/anaconda/lib/python3.4/site-packages/IPython/kernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# This is taking WAAAY too long, expected runtime > 1 hour. Use sparce vector array instead\n",
    "time_check = time.time()\n",
    "for n,q in enumerate(tagged_questions[:10]): # for every tagged questios from 'df.Text'...\n",
    "    for tag in tagset: # take the list of possible tags...\n",
    "        for w in q: # for every tagged word in the question text\n",
    "            if w[1] == tag: # check if the tag matches the one the word is tagged as\n",
    "                df[tag][n] += 1 # if so, increase counter and update the dataframe\n",
    "    if n+1 % 10 == 0:\n",
    "        print(\"Done with first\",n)\n",
    "print(round((time.time() - time_check)/60, 2),\"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finished()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# round up all tags in one list for train data\n",
    "tags_only_train = []\n",
    "for q in tagged_questions:\n",
    "    temp = \"\"\n",
    "    for w in q:\n",
    "        temp += w[1]\n",
    "        temp += \" \"\n",
    "    tags_only_train.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# round up all tags in one list for test data\n",
    "tags_only_test = []\n",
    "for q in tagged_questions_test:\n",
    "    temp = \"\"\n",
    "    for w in q:\n",
    "        temp += w[1]\n",
    "        temp += \" \"\n",
    "    tags_only_test.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finished()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=5, max_features=433)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2698, 433)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train sparce vectorizing\n",
    "arr_train_feature_sparse = vec.fit_transform(pd.Series(tags_only_train))\n",
    "arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "arr_train_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1874, 377)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test sparce vectorizing\n",
    "arr_test_feature_sparse = vec.fit_transform(pd.Series(tags_only_test))\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "arr_test_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Training logistic regression with features in sparce array (arr_train_feature) \n",
    "and labels (df.Category)'''\n",
    "logreg = LogisticRegression()\n",
    "logreg_model_train = logreg.fit(arr_train_feature, df.Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predicting against train features in sparce array\n",
    "logreg_predictions_train = logreg_model_train.predict(arr_train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34875 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "# crossvalidating\n",
    "crossvalidation(logreg, arr_train_feature, df.Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# automatically creates submission with whatever file name you pass as the argument\n",
    "create_submission(\"submission_3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second strategy: simple vectorizer and logistic regression (20.9%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to facilitate testing of variations in CountVecotrizer parameters (min_df, max_features, etc.)\n",
    "def logPredict(vec):\n",
    "    # Train-data sparce vectorizing\n",
    "    arr_train_feature_sparse = vec.fit_transform(df.Text)\n",
    "    arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "    print(\"arr_train_feature.shape:\", arr_train_feature.shape)\n",
    "\n",
    "    # Test-data sparce vectorizing\n",
    "    arr_test_feature_sparse = vec.fit_transform(df_test.Text)\n",
    "    arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "    print(\"arr_test_feature.shape:\", arr_test_feature.shape)\n",
    "\n",
    "    ''' Training logistic regression with features in sparce array (arr_train_feature) \n",
    "    and labels (df.Category)'''\n",
    "    logreg = LogisticRegression()\n",
    "    logreg_model_train = logreg.fit(arr_train_feature, df.Category)\n",
    "\n",
    "    # predicting against train features in sparce array\n",
    "    logreg_predictions_train = logreg_model_train.predict(arr_train_feature)\n",
    "\n",
    "    # predicting against test features in sparce array\n",
    "    logreg_predictions_test = logreg_model_train.predict(arr_test_feature)\n",
    "\n",
    "    # print out crossvalidation scoring results\n",
    "    crossvalidation(logreg, arr_train_feature, df.Category)\n",
    "    \n",
    "    return logreg_predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=5, max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_train_feature.shape: (2698, 640)\n",
      "arr_test_feature.shape: (1874, 640)\n",
      "Accuracy: 0.34875 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "logreg_predictions_test = logPredict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# adding Category column to test set\n",
    "df_test['Category'] = pd.Series(logreg_predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# automatically creates submission with whatever file name you pass as the argument\n",
    "create_submission(\"submission_5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third strategy: bigram, trigram and tetragram vectorizer and logistic regression (22.6%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# notice the ngram_range parameter has \"2, 4\", so ngrams range 2 to 4\n",
    "vec = CountVectorizer(ngram_range=(2, 4), token_pattern=r'\\b\\w+\\b', min_df=5, max_features=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_train_feature.shape: (2698, 640)\n",
      "arr_test_feature.shape: (1874, 640)\n",
      "Accuracy: 0.73904 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "logreg_predictions_test = logPredict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_train_feature.shape: (2698, 640)\n",
      "arr_test_feature.shape: (1874, 640)\n"
     ]
    }
   ],
   "source": [
    "# Train-data sparce vectorizing\n",
    "arr_train_feature_sparse = vec.fit_transform(df.Text)\n",
    "arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "print(\"arr_train_feature.shape:\", arr_train_feature.shape)\n",
    "\n",
    "# Test-data sparce vectorizing\n",
    "arr_test_feature_sparse = vec.fit_transform(df_test.Text)\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "print(\"arr_test_feature.shape:\", arr_test_feature.shape)\n",
    "\n",
    "''' Training logistic regression with features in sparce array (arr_train_feature) \n",
    "and labels (df.Category)'''\n",
    "logreg = LogisticRegression()\n",
    "logreg_model_train = logreg.fit(arr_train_feature, df.Category)\n",
    "\n",
    "# predicting against train features in sparce array\n",
    "logreg_predictions_train = logreg_model_train.predict(arr_train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34875 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "crossvalidation(logreg, arr_train_feature, df.Category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth strategy: same vectorizer but with Naive Bayes (%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to facilitate testing of variations in CountVecotrizer parameters (min_df, max_features, etc.)\n",
    "def NBPredict(vec):\n",
    "    # Train-data sparce vectorizing\n",
    "    arr_train_feature_sparse = vec.fit_transform(df.Text)\n",
    "    arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "    print(\"arr_train_feature.shape:\", arr_train_feature.shape)\n",
    "\n",
    "    # Test-data sparce vectorizing\n",
    "    arr_test_feature_sparse = vec.fit_transform(df_test.Text)\n",
    "    arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "    print(\"arr_test_feature.shape:\", arr_test_feature.shape)\n",
    "\n",
    "    ''' Training logistic regression with features in sparce array (arr_train_feature) \n",
    "    and labels (df.Category)'''\n",
    "    nb = MultinomialNB()\n",
    "    nb_model_train = nb.fit(arr_train_feature, df.Category)\n",
    "\n",
    "    # predicting against train features in sparce array\n",
    "    nb_predictions_train = nb_model_train.predict(arr_train_feature)\n",
    "\n",
    "    # predicting against test features in sparce array\n",
    "    nb_predictions_test = nb_model_train.predict(arr_test_feature)\n",
    "\n",
    "    # print out crossvalidation scoring results\n",
    "    crossvalidation(nb, arr_train_feature, df.Category)\n",
    "    \n",
    "    return nb_predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# notice the ngram_range parameter has \"2, 4\", so ngrams range 2 to 4\n",
    "vec = CountVectorizer(ngram_range=(2, 4), token_pattern=r'\\b\\w+\\b', min_df=5, max_features=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_train_feature.shape: (2698, 640)\n",
      "arr_test_feature.shape: (1874, 640)\n",
      "Accuracy: 0.34686 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "nb_test_predictions = NBPredict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
