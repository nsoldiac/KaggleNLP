{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk, re, string, time, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from textClassification import textAnalysis\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics \n",
    "from sklearn import cross_validation\n",
    "import operator\n",
    "english_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# automate creation of csv for kaggle submission\n",
    "def create_submission(filename, prediction):\n",
    "    pd.DataFrame.to_csv(pd.DataFrame(df_test['Id']).join(pd.DataFrame(prediction)), path_or_buf=filename, header=['Id','Category'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# yell outloud 'process complete'\n",
    "def finished():\n",
    "    os.system(\"say 'process complete'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# automate crossvalidation and printing of results\n",
    "def crossvalidation(model, train_features, real_category):\n",
    "    scores = cross_validation.cross_val_score(model, train_features, real_category, cv=5)\n",
    "    print(\"Accuracy: %0.5f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"newtrain.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"newtest.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allText = \"\"\n",
    "for i in df.Text:\n",
    "    allText += i + '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('allText.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.write(allText)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing functions from Text Analysis assignement on Yahoo questions text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"allText.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analysis = textAnalysis(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NP parsing.\n",
      "Done with NP parsing.\n",
      "Done with FD of NPs\n",
      "Extracting nouns...\n",
      "Done extracting nouns\n",
      "Done running Lesk on nouns\n",
      "Done creating FD on lemmas\n",
      "\n",
      "Most frequent noun phrases:\n",
      "---------------------------\n",
      ".how do                 91   \n",
      "br & gt                 27   \n",
      ".what do                22   \n",
      "my boyfriend            19   \n",
      "a guy                   19   \n",
      "my comput               16   \n",
      "the world               15   \n",
      "a girl                  14   \n",
      ".whi do                 11   \n",
      ".i am                   9    \n",
      ".i want                 9    \n",
      "my husband              9    \n",
      "\n",
      "\n",
      "Most frequent lemmas from NPs and their hyponyms:\n",
      "-------------------------------------------------\n",
      "can             194      (containerful.n.01)\n",
      "get             184      (return.n.11)\n",
      "know            156      (knowing.n.01)\n",
      "find            89       (act.n.02)\n",
      "need            77       (psychological_feature.n.01)\n",
      "want            70       (need.n.01)\n",
      "people          58       (group.n.01)\n",
      "someone         51       (organism.n.01)\n",
      "someone         51       (causal_agent.n.01)\n",
      "help            50       (activity.n.01)\n",
      "use             43       (custom.n.01)\n",
      "way             42       (category.n.02)\n",
      "\n",
      "\n",
      "Most frequent hyponyms:\n",
      "-----------------------\n",
      "person.n.01                    8    \n",
      "activity.n.01                  7    \n",
      "time_period.n.01               7    \n",
      "collection.n.01                4    \n",
      "computer.n.01                  3    \n",
      "woman.n.01                     3    \n",
      "content.n.05                   3    \n",
      "atmosphere.n.01                3    \n",
      "language_unit.n.01             3    \n",
      "location.n.01                  3    \n",
      "legal_document.n.01            3    \n",
      "physical_phenomenon.n.01       3    \n"
     ]
    }
   ],
   "source": [
    "# tokenize sentences and words an run them through a POS tagger\n",
    "sentences,tagged_sents = analysis.gen_corpus_prep()\n",
    "\n",
    "# generate noun phrases and proper nouns\n",
    "nps,nps_strings,fd_nps = analysis.chunker(sentences,tagged_sents)\n",
    "\n",
    "# deduce synsets through lesk algorithm and calculate distance between synsets\n",
    "final_list,hyponyms = analysis.lesk_comparison(nps,sentences)\n",
    "\n",
    "# print final report\n",
    "analysis.final_report(fd_nps,final_list,hyponyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_dict = {1:\"Business&Finance\",2:\"Computers&Internet\",3:\"Entertainment&Music\",4:\"Family&Relationships\",5:\"Education&Reference\",6:\"Health\",7:\"Science&Mathematics\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating feature for lengths of text and converting to pandas series\n",
    "lengths = []\n",
    "for i in df.Text.iteritems():\n",
    "    lengths.append(len(i[1]))\n",
    "lengths = pd.Series(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# appending lengths feature to dataframe\n",
    "df['lengths'] = lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# aggregate all text\n",
    "all_text = []\n",
    "for i in df.Text:\n",
    "    all_text.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# aggregate all text and tokenize it\n",
    "tok_sent = []\n",
    "for i in df.Text:\n",
    "    tok_sent.append(nltk.word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pos_tag every word\n",
    "pos_sent = []\n",
    "for i in tok_sent:\n",
    "    pos_sent.append(nltk.pos_tag(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation and Testing of Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Strategy: Count occurrence of every tag (17.6%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating list with all tags from upenn_tagset\n",
    "tagset = ['$', \"''\", '(', ')', ',', '--', '.', ':','CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating columns of zeroes for every possible tag\n",
    "for i in tagset:\n",
    "    df[i] = pd.Series(np.zeros(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenizing and tagging every question training set\n",
    "tagged_questions = []\n",
    "for i in df.Text:\n",
    "    tagged_questions.append(nltk.pos_tag(nltk.word_tokenize(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenizing and tagging every question for test set\n",
    "tagged_questions_test = []\n",
    "for i in df_test.Text:\n",
    "    tagged_questions_test.append(nltk.pos_tag(nltk.word_tokenize(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenqin/anaconda3/lib/python3.4/site-packages/IPython/kernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# This is taking WAAAY too long, expected runtime > 1 hour. Use sparce vector array instead\n",
    "time_check = time.time()\n",
    "for n,q in enumerate(tagged_questions[:10]): # for every tagged questios from 'df.Text'...\n",
    "    for tag in tagset: # take the list of possible tags...\n",
    "        for w in q: # for every tagged word in the question text\n",
    "            if w[1] == tag: # check if the tag matches the one the word is tagged as\n",
    "                df[tag][n] += 1 # if so, increase counter and update the dataframe\n",
    "    if n+1 % 10 == 0:\n",
    "        print(\"Done with first\",n)\n",
    "print(round((time.time() - time_check)/60, 2),\"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finished()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# round up all tags in one list for train data\n",
    "tags_only_train = []\n",
    "for q in tagged_questions:\n",
    "    temp = \"\"\n",
    "    for w in q:\n",
    "        temp += w[1]\n",
    "        temp += \" \"\n",
    "    tags_only_train.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# round up all tags in one list for test data\n",
    "tags_only_test = []\n",
    "for q in tagged_questions_test:\n",
    "    temp = \"\"\n",
    "    for w in q:\n",
    "        temp += w[1]\n",
    "        temp += \" \"\n",
    "    tags_only_test.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finished()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=5, max_features=433)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2698, 433)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train sparce vectorizing\n",
    "arr_train_feature_sparse = vec.fit_transform(pd.Series(tags_only_train))\n",
    "arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "arr_train_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_train_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1874, 377)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test sparce vectorizing\n",
    "arr_test_feature_sparse = vec.fit_transform(pd.Series(tags_only_test))\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "arr_test_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Training logistic regression with features in sparce array (arr_train_feature) \n",
    "and labels (df.Category)'''\n",
    "logreg = LogisticRegression()\n",
    "logreg_model_train = logreg.fit(arr_train_feature, df.Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predicting against train features in sparce array\n",
    "logreg_predictions_train = logreg_model_train.predict(arr_train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.30913 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "# crossvalidating\n",
    "crossvalidation(logreg, arr_train_feature, df.Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_submission() missing 1 required positional argument: 'prediction'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-136-a99997151741>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# automatically creates submission with whatever file name you pass as the argument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcreate_submission\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"submission_3.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: create_submission() missing 1 required positional argument: 'prediction'"
     ]
    }
   ],
   "source": [
    "# automatically creates submission with whatever file name you pass as the argument\n",
    "create_submission(\"submission_3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second strategy: simple vectorizer and logistic regression (20.9%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to facilitate testing of variations in CountVecotrizer parameters (min_df, max_features, etc.)\n",
    "def logPredict(vec, train_set, other_set, scoring, return_train=True):\n",
    "    # Train-data sparce vectorizing\n",
    "    arr_train_feature_sparse = vec.fit_transform(train_set.Text)\n",
    "    arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "    print(\"arr_train_feature.shape:\", arr_train_feature.shape)\n",
    "\n",
    "    # Other data sparce vectorizing\n",
    "    arr_other_feature_sparse = vec.transform(other_set.Text)\n",
    "    arr_other_feature = arr_other_feature_sparse.toarray()\n",
    "    print(\"arr_other_feature.shape:\", arr_other_feature.shape,'\\n')\n",
    "\n",
    "    ''' Training model with features in sparce array (arr_train_feature) and labels'''\n",
    "    log_reg = LogisticRegression()\n",
    "    logreg_model_train = log_reg.fit(arr_train_feature, train_set.Category)\n",
    "\n",
    "    # print out crossvalidation or scoring results\n",
    "    if scoring == 'cv':\n",
    "        crossvalidation(log_reg, arr_train_feature, train_set.Category)\n",
    "    elif scoring == 'scoring': \n",
    "        logreg_predictions_other = logreg_model_train.predict(arr_other_feature)\n",
    "        print(accuracy_score(other_set.Category, logreg_predictions_other))\n",
    "    else:\n",
    "        print(\"Can't recognize scoring parameter.\")\n",
    "    \n",
    "    if return_train:\n",
    "        # predicting against train features in sparce array\n",
    "        logreg_predictions_train = logreg_model_train.predict(arr_train_feature)\n",
    "        print('\\nReturning predictions for train set...')\n",
    "        return logreg_predictions_train\n",
    "    else:\n",
    "        print('Returning predictions for other (dev or test) set...')\n",
    "        # predicting against dev features in sparce array\n",
    "        logreg_predictions_other = logreg_model_train.predict(arr_other_feature)\n",
    "        return logreg_predictions_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=5, max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_train_feature.shape: (2698, 640)\n",
      "arr_test_feature.shape: (1874, 640)\n",
      "Accuracy: 0.34875 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "logreg_predictions_test = logPredict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# adding Category column to test set\n",
    "df_test['Category'] = pd.Series(logreg_predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# automatically creates submission with whatever file name you pass as the argument\n",
    "create_submission(\"submission_5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third strategy: bigram, trigram and tetragram vectorizer and logistic regression (22.6%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# notice the ngram_range parameter has \"2, 4\", so ngrams range 2 to 4\n",
    "vec = CountVectorizer(ngram_range=(2, 4), token_pattern=r'\\b\\w+\\b', min_df=5, max_features=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_train_feature.shape: (2698, 640)\n",
      "arr_test_feature.shape: (1874, 640)\n",
      "Accuracy: 0.73904 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "logreg_predictions_test = logPredict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_train_feature.shape: (2698, 640)\n",
      "arr_test_feature.shape: (1874, 640)\n"
     ]
    }
   ],
   "source": [
    "# Train-data sparce vectorizing\n",
    "arr_train_feature_sparse = vec.fit_transform(df.Text)\n",
    "arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "print(\"arr_train_feature.shape:\", arr_train_feature.shape)\n",
    "\n",
    "# Test-data sparce vectorizing\n",
    "arr_test_feature_sparse = vec.fit_transform(df_test.Text)\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "print(\"arr_test_feature.shape:\", arr_test_feature.shape)\n",
    "\n",
    "''' Training logistic regression with features in sparce array (arr_train_feature) \n",
    "and labels (df.Category)'''\n",
    "logreg = LogisticRegression()\n",
    "logreg_model_train = logreg.fit(arr_train_feature, df.Category)\n",
    "\n",
    "# predicting against train features in sparce array\n",
    "logreg_predictions_train = logreg_model_train.predict(arr_train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34875 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "crossvalidation(logreg, arr_train_feature, df.Category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth strategy: same vectorizer but with Naive Bayes (%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to facilitate testing of variations in CountVecotrizer parameters (min_df, max_features, etc.)\n",
    "def NBPredict(model, vec, train_set, other_set, scoring, num_neighbors = 1, gamma= 0.001):\n",
    "    '''\n",
    "    model -  LogisticRegression, MultinomialNB, KNeighborsClassifier, svm\n",
    "    '''\n",
    "    # Train-data sparce vectorizing\n",
    "    arr_train_feature_sparse = vec.fit_transform(train_set.Text)\n",
    "    arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "    print(\"arr_train_feature.shape:\", arr_train_feature.shape)\n",
    "\n",
    "    # Other data sparce vectorizing\n",
    "    arr_other_feature_sparse = vec.transform(other_set.Text)\n",
    "    arr_other_feature = arr_other_feature_sparse.toarray()\n",
    "    print(\"arr_other_feature.shape:\", arr_other_feature.shape,'\\n')\n",
    "\n",
    "    ''' Training model with features in sparce array (arr_train_feature) and labels (df_.Category)'''\n",
    "    if model == KNeighborsClassifier:\n",
    "        nb = model(n_neighbors = num_neighbors)\n",
    "    elif model == svm:\n",
    "        nb = model.SVC(gamma=gamma, C=100)\n",
    "    else:\n",
    "        nb = model()\n",
    "   \n",
    "    nb_model_train = nb.fit(arr_train_feature, train_set.Category)    \n",
    "\n",
    "    # predicting against dev features in sparce array\n",
    "    nb_predictions_other = nb_model_train.predict(arr_other_feature)\n",
    "    \n",
    "    # print out crossvalidation or scoring results\n",
    "    if scoring == 'cv':\n",
    "        crossvalidation(nb, arr_train_feature, train_set.Category)\n",
    "    elif scoring == 'dev': \n",
    "        print(accuracy_score(other_set.Category, nb_predictions_other))\n",
    "        \n",
    "    return nb_predictions_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# notice the ngram_range parameter has \"2, 4\", so ngrams range 2 to 4\n",
    "vec = CountVectorizer(ngram_range=(2, 4), token_pattern=r'\\b\\w+\\b', min_df=5, max_features=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nb_train_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-38107103e180>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCategory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_train_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Predictions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nb_train_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(df.Category).join(pd.DataFrame(nb_train_predictions, columns=['Predictions'])).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fifth strategy: Tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(min_df=1, max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_train_feature.shape: (2698, 1000)\n",
      "arr_test_feature.shape: (1874, 1000)\n",
      "Accuracy: 0.50072 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "tfid_test_predictions = logPredict(tfidf_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_train_feature.shape: (2698, 1000)\n",
      "arr_test_feature.shape: (1874, 1000) \n",
      "\n",
      "Accuracy: 0.45546 (+/- 0.04)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2, ..., 1, 1, 4])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NBPredict(tfidf_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_submission('submission_6.csv', tfid_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proper Nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**develop training, dev, testing sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>$</th>\n",
       "      <th>''</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>,</th>\n",
       "      <th>--</th>\n",
       "      <th>.</th>\n",
       "      <th>:</th>\n",
       "      <th>...</th>\n",
       "      <th>VBD</th>\n",
       "      <th>VBG</th>\n",
       "      <th>VBN</th>\n",
       "      <th>VBP</th>\n",
       "      <th>VBZ</th>\n",
       "      <th>WDT</th>\n",
       "      <th>WP</th>\n",
       "      <th>WP$</th>\n",
       "      <th>WRB</th>\n",
       "      <th>``</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>can you give me tips on how to get over an ex?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>waht are sources of vitamin d?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>structural framework of internet?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>how do i get an ip hider for my computer to he...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>when sign makers go on strike</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                               Text  $  ''  (  )  \\\n",
       "0         4     can you give me tips on how to get over an ex?  0   0  0  0   \n",
       "1         6                     waht are sources of vitamin d?  0   0  0  0   \n",
       "2         2                  structural framework of internet?  0   0  0  0   \n",
       "3         2  how do i get an ip hider for my computer to he...  0   0  0  0   \n",
       "4         1                      when sign makers go on strike  0   0  0  0   \n",
       "\n",
       "   ,  --  .  : ...  VBD  VBG  VBN  VBP  VBZ  WDT  WP  WP$  WRB  ``  \n",
       "0  0   0  0  0 ...    0    0    0    0    0    0   0    0    0   0  \n",
       "1  0   0  0  0 ...    0    0    0    0    0    0   0    0    0   0  \n",
       "2  0   0  0  0 ...    0    0    0    0    0    0   0    0    0   0  \n",
       "3  0   0  0  0 ...    0    0    0    0    0    0   0    0    0   0  \n",
       "4  0   0  0  0 ...    0    0    0    0    0    0   0    0    0   0  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_index = np.random.permutation(df.index)\n",
    "df_shuffled = df.ix[random_index]\n",
    "df_shuffled.reset_index(drop=True, inplace=True)\n",
    "df_shuffled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 2698\n",
      "Columns: 47\n"
     ]
    }
   ],
   "source": [
    "rows, columns = df_shuffled.shape\n",
    "print(\"Rows:\", rows)\n",
    "print(\"Columns:\", columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**80/20 split for training/ dev**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = round(rows*.8)\n",
    "dev_size   = round(rows*.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (2159, 47)\n",
      "dev: (540, 47)\n"
     ]
    }
   ],
   "source": [
    "df_train = df_shuffled.loc[:train_size]\n",
    "print('train:',df_train.shape)\n",
    "\n",
    "df_dev = df_shuffled.loc[train_size:dev_size+train_size].reset_index(drop=True)\n",
    "print('dev:',df_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Implement voting system between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " ...]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vote_winner(pair):\n",
    "    dic = {}\n",
    "    for i in range(len(pair)):\n",
    "        num = pair[i]\n",
    "        if num in dic:\n",
    "            dic[num] += 1\n",
    "        else:\n",
    "            dic[num] = 1\n",
    "    return sorted(dic.items(), key=operator.itemgetter(1), reverse=True)[0][0]\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## counting # of \"I\"\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-269-5e5d721667f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0marr_train_feature_sparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mText\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0marr_train_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr_train_feature_sparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mdf_train_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr_test_feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/wenqin/anaconda3/lib/python3.4/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2125\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2126\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2127\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2129\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/wenqin/anaconda3/lib/python3.4/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2203\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2204\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2205\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/wenqin/anaconda3/lib/python3.4/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2361\u001b[0m             \u001b[1;31m# turn me into an ndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2362\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2363\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2364\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/wenqin/anaconda3/lib/python3.4/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_index\u001b[1;34m(data, index, copy)\u001b[0m\n\u001b[0;32m   2577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2578\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2579\u001b[1;33m         raise ValueError('Length of values does not match length of '\n\u001b[0m\u001b[0;32m   2580\u001b[0m                          'index')\n\u001b[0;32m   2581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "tok_sent_train = []\n",
    "for i in df_train.Text:\n",
    "    tok_sent_train.append(nltk.word_tokenize(i))\n",
    "    \n",
    "num_i_train = []\n",
    "for i in tok_sent_train:\n",
    "    count = 0\n",
    "    for j in i:\n",
    "        if j == \"i\":\n",
    "            count += 1\n",
    "    num_i_train.append(count)\n",
    "    \n",
    "# num_i = pd.Series(num_i)\n",
    "# df_train['num_i'] = num_i\n",
    "\n",
    "df_train_features = pd.DataFrame({'num_i': num_i_train})\n",
    "\n",
    "#Error: cannot join the feature columns...\n",
    "\n",
    "vec_count = tfidf_vec = TfidfVectorizer(min_df=1, max_features=1000)\n",
    "arr_train_feature_sparse = vec_count.fit_transform(df_train.Text)\n",
    "arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "df_train_features['count'] = arr_test_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_i\n",
       "0      0\n",
       "1      0\n",
       "2      1\n",
       "3      0\n",
       "4      0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dev**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok_sent_dev = []\n",
    "for i in df_dev.Text:\n",
    "    tok_sent_dev.append(nltk.word_tokenize(i))\n",
    "    \n",
    "num_i_dev = []\n",
    "for i in tok_sent_dev:\n",
    "    count = 0\n",
    "    for j in i:\n",
    "        if j == \"i\":\n",
    "            count += 1\n",
    "    num_i_dev.append(count)\n",
    "    \n",
    "# num_i_dev = pd.Series(num_i)\n",
    "# df_dev['num_i'] = num_i\n",
    "\n",
    "\n",
    "df_dev_features = pd.DataFrame({'num_i': num_i_dev})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_i\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      1\n",
      "4      0 \n",
      "Shape: (540, 1)\n"
     ]
    }
   ],
   "source": [
    "print(df_dev_features.head(),'\\nShape:',df_dev_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb_model = nb.fit(df_train_features, df_train.Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_predictions = nb_model.predict(df_dev_features)\n",
    "nb_predictions[:10]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540\n",
      "540\n"
     ]
    }
   ],
   "source": [
    "print(len(df_dev.Category))\n",
    "print(len(nb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28888888888888886"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(df_dev.Category, nb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.28393 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "crossvalidation(nb_model, df_train_features, df_train.Category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(min_df=1, max_features=1190, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test on dev set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_train_feature.shape: (2159, 1000)\n",
      "arr_other_feature.shape: (540, 1000) \n",
      "\n",
      "0.42037037037\n",
      "arr_train_feature.shape: (2159, 1000)\n",
      "arr_other_feature.shape: (540, 1000) \n",
      "\n",
      "0.47037037037\n",
      "arr_train_feature.shape: (2159, 1000)\n",
      "arr_other_feature.shape: (540, 1000) \n",
      "\n",
      "0.3\n",
      "arr_train_feature.shape: (2159, 1000)\n",
      "arr_other_feature.shape: (540, 1000) \n",
      "\n",
      "Accuracy: 0.50256 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "dev_predictions_nb = NBPredict(MultinomialNB, tfidf_vec, df_train, df_dev, 'dev')\n",
    "dev_predictions_logreg = NBPredict(LogisticRegression, tfidf_vec, df_train, df_dev, 'dev')\n",
    "dev_predictions_KNN = NBPredict(KNeighborsClassifier, tfidf_vec, df_train, df_dev, 'dev', num_neighbors=3)\n",
    "dev_predictions_svm = NBPredict(svm, tfidf_vec, df_train, df_dev, 'cv', gamma=0.005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voted_results_dev = [vote_winner(pair) for pair in zip(dev_predictions_logreg, dev_predictions_nb, dev_predictions_svm)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46111111111111114"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(voted_results_dev, df_dev.Category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**submit test set to Kaggle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_train_feature.shape: (2698, 1000)\n",
      "arr_other_feature.shape: (1874, 1000) \n",
      "\n",
      "Accuracy: 0.45546 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "test_predictions_nb = NBPredict(MultinomialNB, tfidf_vec, df, df_test, 'cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_submission(\"submission_7.csv\", test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_train_feature.shape: (2698, 1000)\n",
      "arr_other_feature.shape: (1874, 1000) \n",
      "\n",
      "Accuracy: 0.50072 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "test_predictions_logreg = NBPredict(LogisticRegression, tfidf_vec, df, df_test, 'cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_train_feature.shape: (2698, 1000)\n",
      "arr_other_feature.shape: (1874, 1000) \n",
      "\n",
      "Accuracy: 0.30539 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "#this one needs tuning...\n",
    "test_predictions_KNN = NBPredict(KNeighborsClassifier, tfidf_vec, df, df_test, 'cv', num_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_train_feature.shape: (2698, 1000)\n",
      "arr_other_feature.shape: (1874, 1000) \n",
      "\n",
      "Accuracy: 0.50663 (+/- 0.03)\n",
      "Returning predictions for other (dev or test) set...\n"
     ]
    }
   ],
   "source": [
    "# tried gamma = 0.001, 0.003, 0.005 and 0.007. 0.005 has the best result\n",
    "test_predictions_svm = NBPredict(svm, tfidf_vec, df, df_test, 'cv', gamma=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_submission(\"submission_8.csv\", test_predictions_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply voting system between three models: logreg, nb, and svm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voted_results = [vote_winner(pair) for pair in zip(test_predictions_logreg, test_predictions_nb, test_predictions_svm)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_submission(\"submission_9.csv\", voted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
